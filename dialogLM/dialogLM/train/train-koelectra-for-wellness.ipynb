{"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"NCpnYj2bfXvG"},"source":["# Wellness 심리 상담 데이터에 대한 KoELECTRA 학습 Question & Answer \n","데이터와 클래스 셋으로 이루어져, 질의 데이터가 들어왔을 때, Answer 클래스를 예측하도록 학습"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"EN1oD-YEfukE"},"source":["## 1.Google Drive 연동\n","- 모델 파일과 학습 데이터가 저장 되어있는 구글 드라이브의 디렉토리와 Colab을 연동.  \n","- 좌측상단 메뉴에서 런타임-> 런타임 유형 변경 -> 하드웨어 가속기 -> GPU 선택 후 저장"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"7TlLDiAJf0Zz"},"source":["### 1.1 GPU 연동 확인"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":372},"colab_type":"code","executionInfo":{"elapsed":3384,"status":"ok","timestamp":1595810235695,"user":{"displayName":"김성환","photoUrl":"","userId":"17497395371430681608"},"user_tz":-540},"id":"Q1qPeOGVfkb_","outputId":"7d1f0d28-4beb-41e5-ff7f-216a2476a9e9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mon Jul 27 00:37:13 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 450.51.05    Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   47C    P0    34W / 250W |   1275MiB / 16280MiB |      0%      Default |\n","|                               |                      |                 ERR! |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9tW2_JPaf79o"},"source":["### 1.2 Google Drive 연동\n","아래 코드를 실행후 나오는 URL을 클릭하여 나오는 인증 코드 입력"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","executionInfo":{"elapsed":3311,"status":"ok","timestamp":1595810235700,"user":{"displayName":"김성환","photoUrl":"","userId":"17497395371430681608"},"user_tz":-540},"id":"7voF7JHtf4J5","outputId":"071be4da-b4e9-4fc4-9bcc-63bf9cdec271"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"T4MqQlXugAdI"},"source":["**Colab 디렉토리 아래 dialogLM 경로 확인**\n","\n","\n"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":87},"colab_type":"code","executionInfo":{"elapsed":5191,"status":"ok","timestamp":1595810237630,"user":{"displayName":"김성환","photoUrl":"","userId":"17497395371430681608"},"user_tz":-540},"id":"mdqgpGkLgGoc","outputId":"092b3828-3c9e-44ba-9883-8e0cfe882ed6"},"outputs":[{"name":"stdout","output_type":"stream","text":[" BERT_X\t\t      'fastprogress example.ipynb'    NarrativeKoGPT2\n"," Data\t\t       KorQuAD-beginner\t\t      ReforBERT\n"," dialogLM\t       korquad-finetuing.ipynb\n"," EnlipleBERTFintuing   korquad-finetuing-ver2.ipynb\n"]}],"source":["!ls drive/'My Drive'/'Colab Notebooks'/"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"wA36dTxFgMA8"},"source":["**필요 패키지 설치**"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":463},"colab_type":"code","executionInfo":{"elapsed":8621,"status":"ok","timestamp":1595810241118,"user":{"displayName":"김성환","photoUrl":"","userId":"17497395371430681608"},"user_tz":-540},"id":"OdbnMf8NjSHN","outputId":"92ca46c1-924f-43c1-bf77-a01d25234931"},"outputs":[{"name":"stdout","output_type":"stream","text":["Active code page: 65001\n","Requirement already satisfied: kogpt2-transformers in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (0.4.0)\n","Requirement already satisfied: torch>=1.1.0 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from kogpt2-transformers) (2.1.2)\n","Collecting transformers>=4.0.0 (from kogpt2-transformers)\n","  Using cached transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n","Requirement already satisfied: tokenizers>=0.7.0 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from kogpt2-transformers) (0.7.0)\n","Requirement already satisfied: filelock in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from torch>=1.1.0->kogpt2-transformers) (3.13.1)\n","Requirement already satisfied: typing-extensions in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from torch>=1.1.0->kogpt2-transformers) (4.9.0)\n","Requirement already satisfied: sympy in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from torch>=1.1.0->kogpt2-transformers) (1.12)\n","Requirement already satisfied: networkx in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from torch>=1.1.0->kogpt2-transformers) (3.1)\n","Requirement already satisfied: jinja2 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from torch>=1.1.0->kogpt2-transformers) (3.1.2)\n","Requirement already satisfied: fsspec in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from torch>=1.1.0->kogpt2-transformers) (2023.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from transformers>=4.0.0->kogpt2-transformers) (0.20.1)\n","Requirement already satisfied: numpy>=1.17 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from transformers>=4.0.0->kogpt2-transformers) (1.24.4)\n","Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from transformers>=4.0.0->kogpt2-transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from transformers>=4.0.0->kogpt2-transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from transformers>=4.0.0->kogpt2-transformers) (2023.12.25)\n","Requirement already satisfied: requests in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from transformers>=4.0.0->kogpt2-transformers) (2.31.0)\n","Collecting tokenizers>=0.7.0 (from kogpt2-transformers)\n","  Using cached tokenizers-0.15.0-cp38-none-win_amd64.whl.metadata (6.8 kB)\n","Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from transformers>=4.0.0->kogpt2-transformers) (0.4.1)\n","Requirement already satisfied: tqdm>=4.27 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from transformers>=4.0.0->kogpt2-transformers) (4.66.1)\n","Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from tqdm>=4.27->transformers>=4.0.0->kogpt2-transformers) (0.4.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from jinja2->torch>=1.1.0->kogpt2-transformers) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from requests->transformers>=4.0.0->kogpt2-transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from requests->transformers>=4.0.0->kogpt2-transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from requests->transformers>=4.0.0->kogpt2-transformers) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from requests->transformers>=4.0.0->kogpt2-transformers) (2023.11.17)\n","Requirement already satisfied: mpmath>=0.19 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from sympy->torch>=1.1.0->kogpt2-transformers) (1.3.0)\n","Using cached transformers-4.36.2-py3-none-any.whl (8.2 MB)\n","Using cached tokenizers-0.15.0-cp38-none-win_amd64.whl (2.2 MB)\n","Installing collected packages: tokenizers, transformers\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.7.0\n","    Uninstalling tokenizers-0.7.0:\n","      Successfully uninstalled tokenizers-0.7.0\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 2.10.0\n","    Uninstalling transformers-2.10.0:\n","      Successfully uninstalled transformers-2.10.0\n","Successfully installed tokenizers-0.15.0 transformers-4.36.2\n"]},{"name":"stderr","output_type":"stream","text":["ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","s2s-ft 0.0.1 requires transformers<=2.10.0, but you have transformers 4.36.2 which is incompatible.\n"]},{"name":"stdout","output_type":"stream","text":["Active code page: 65001\n","Requirement already satisfied: kobert-transformers in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (0.4.1)\n","Requirement already satisfied: torch>=1.1.0 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from kobert-transformers) (2.1.2)\n","Requirement already satisfied: transformers>=2.9.1 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from kobert-transformers) (4.36.2)\n","Requirement already satisfied: filelock in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from torch>=1.1.0->kobert-transformers) (3.13.1)\n","Requirement already satisfied: typing-extensions in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from torch>=1.1.0->kobert-transformers) (4.9.0)\n","Requirement already satisfied: sympy in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from torch>=1.1.0->kobert-transformers) (1.12)\n","Requirement already satisfied: networkx in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from torch>=1.1.0->kobert-transformers) (3.1)\n","Requirement already satisfied: jinja2 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from torch>=1.1.0->kobert-transformers) (3.1.2)\n","Requirement already satisfied: fsspec in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from torch>=1.1.0->kobert-transformers) (2023.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from transformers>=2.9.1->kobert-transformers) (0.20.1)\n","Requirement already satisfied: numpy>=1.17 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from transformers>=2.9.1->kobert-transformers) (1.24.4)\n","Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from transformers>=2.9.1->kobert-transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from transformers>=2.9.1->kobert-transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from transformers>=2.9.1->kobert-transformers) (2023.12.25)\n","Requirement already satisfied: requests in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from transformers>=2.9.1->kobert-transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from transformers>=2.9.1->kobert-transformers) (0.15.0)\n","Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from transformers>=2.9.1->kobert-transformers) (0.4.1)\n","Requirement already satisfied: tqdm>=4.27 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from transformers>=2.9.1->kobert-transformers) (4.66.1)\n","Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from tqdm>=4.27->transformers>=2.9.1->kobert-transformers) (0.4.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from jinja2->torch>=1.1.0->kobert-transformers) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from requests->transformers>=2.9.1->kobert-transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from requests->transformers>=2.9.1->kobert-transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from requests->transformers>=2.9.1->kobert-transformers) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from requests->transformers>=2.9.1->kobert-transformers) (2023.11.17)\n","Requirement already satisfied: mpmath>=0.19 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from sympy->torch>=1.1.0->kobert-transformers) (1.3.0)\n","Active code page: 65001\n","Requirement already satisfied: torch in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (2.1.2)\n","Requirement already satisfied: filelock in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from torch) (3.13.1)\n","Requirement already satisfied: typing-extensions in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from torch) (4.9.0)\n","Requirement already satisfied: sympy in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from torch) (3.1)\n","Requirement already satisfied: jinja2 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from torch) (3.1.2)\n","Requirement already satisfied: fsspec in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from torch) (2023.12.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from jinja2->torch) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from sympy->torch) (1.3.0)\n"]}],"source":["# !pip install -r drive/'My Drive'/'Colab Notebooks'/dialogLM/requirements.txt\n","!pip install kogpt2-transformers\n","!pip install kobert-transformers\n","!pip install torch"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Active code page: 65001\n","Requirement already satisfied: tokenizers in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (0.15.0)\n","Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from tokenizers) (0.20.1)\n","Requirement already satisfied: filelock in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (3.13.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2023.12.2)\n","Requirement already satisfied: requests in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.66.1)\n","Requirement already satisfied: pyyaml>=5.1 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.9.0)\n","Requirement already satisfied: packaging>=20.9 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (23.2)\n","Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub<1.0,>=0.16.4->tokenizers) (0.4.6)\n","Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2023.11.17)\n","Active code page: 65001\n","Requirement already satisfied: transformers in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (4.36.2)\n","Requirement already satisfied: filelock in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from transformers) (0.20.1)\n","Requirement already satisfied: numpy>=1.17 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from transformers) (1.24.4)\n","Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from transformers) (0.15.0)\n","Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from transformers) (0.4.1)\n","Requirement already satisfied: tqdm>=4.27 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n","Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n","Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from requests->transformers) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages (from requests->transformers) (2023.11.17)\n"]}],"source":["!pip install tokenizers\n","!pip install transformers"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"i51bCZyCjZzy"},"source":["## KoBERT QA Training"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"UFSAkW6MkZb7"},"source":["**Path 추가**"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{},"colab_type":"code","executionInfo":{"elapsed":8577,"status":"ok","timestamp":1595810241123,"user":{"displayName":"김성환","photoUrl":"","userId":"17497395371430681608"},"user_tz":-540},"id":"BRNM3Al6kdCI"},"outputs":[],"source":["import sys\n","sys.path.append('drive/My Drive/Colab Notebooks/')\n","sys.path.append('drive/My Drive/Colab Notebooks/dialogLM')"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import sys\n","sys.path.append('C:\\\\sqlite\\\\mysql\\\\code\\\\AI\\\\FINAL_project\\\\dialogLM')"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"KrFfFlV8jfcg"},"source":["### 2.1 import package"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{},"colab_type":"code","executionInfo":{"elapsed":8536,"status":"ok","timestamp":1595810241126,"user":{"displayName":"김성환","photoUrl":"","userId":"17497395371430681608"},"user_tz":-540},"id":"TS7um4UEsGdi"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","from IPython.display import display\n","from tqdm import tqdm\n","\n","import torch\n","from transformers import (\n","  AdamW,\n","  ElectraConfig,\n","  ElectraTokenizer\n",")\n","from torch.utils.data import dataloader\n","from dialogLM.dataloader.wellness import WellnessTextClassificationDataset\n","from dialogLM.model.koelectra import koElectraForSequenceClassification"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","executionInfo":{"elapsed":8411,"status":"ok","timestamp":1595810241129,"user":{"displayName":"김성환","photoUrl":"","userId":"17497395371430681608"},"user_tz":-540},"id":"iHvrtMJaoV_Z","outputId":"3aa222e6-d76f-418d-9e8f-b618ef6bd7cf"},"outputs":[{"data":{"text/plain":["True"]},"execution_count":18,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["torch.cuda.is_available()"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"JuH_ldfXsNn-"},"source":["**Train 함수**"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{},"colab_type":"code","executionInfo":{"elapsed":8397,"status":"ok","timestamp":1595810241132,"user":{"displayName":"김성환","photoUrl":"","userId":"17497395371430681608"},"user_tz":-540},"id":"ELfj8XjhsM0n"},"outputs":[],"source":["def train(epoch, model, optimizer, train_loader, save_step, save_ckpt_path, train_step = 0):\n","    losses = []\n","    train_start_index = train_step+1 if train_step != 0 else 0\n","    total_train_step = len(train_loader)\n","    model.train()\n","\n","    with tqdm(total= total_train_step, desc=f\"Train({epoch})\") as pbar:\n","        pbar.update(train_step)\n","        for i, data in enumerate(train_loader, train_start_index):\n","            optimizer.zero_grad()\n","\n","            '''\n","            inputs = {'input_ids': batch[0],\n","                      'attention_mask': batch[1],\n","                      'bias_labels': batch[3],\n","                      'hate_labels': batch[4]}\n","            if self.args.model_type != 'distilkobert':\n","              inputs['token_type_ids'] = batch[2]\n","            '''\n","            inputs = {'input_ids': data['input_ids'],\n","                      'attention_mask': data['attention_mask'],\n","                      'labels': data['labels']\n","                      }\n","            outputs = model(**inputs)\n","\n","            loss = outputs[0]\n","\n","            losses.append(loss.item())\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            pbar.update(1)\n","            pbar.set_postfix_str(f\"Loss: {loss.item():.3f} ({np.mean(losses):.3f})\")\n","\n","            if i >= total_train_step or i % save_step == 0:\n","                torch.save({\n","                    'epoch': epoch,  # 현재 학습 epoch\n","                    'model_state_dict': model.state_dict(),  # 모델 저장\n","                    'optimizer_state_dict': optimizer.state_dict(),  # 옵티마이저 저장\n","                    'loss': loss.item(),  # Loss 저장\n","                    'train_step': i,  # 현재 진행한 학습\n","                    'total_train_step': len(train_loader)  # 현재 epoch에 학습 할 총 train step\n","                }, save_ckpt_path)\n","\n","    return np.mean(losses)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"mZuWyjJEjxsF"},"source":["### KoBERT Question & Answer Training for Wellness dataset"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"colab_type":"code","executionInfo":{"elapsed":13354992,"status":"ok","timestamp":1595823587760,"user":{"displayName":"김성환","photoUrl":"","userId":"17497395371430681608"},"user_tz":-540},"id":"Y9vwUZQ9j0DN","outputId":"3d38b90a-b730-46a3-f293-00397b246824"},"outputs":[{"name":"stderr","output_type":"stream","text":["pytorch_model.bin: 100%|██████████| 443M/443M [00:26<00:00, 16.6MB/s] \n","Some weights of koElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","c:\\Users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","Train(0):   0%|          | 0/327 [00:47<?, ?it/s]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[11], line 56\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epoch):\n\u001b[0;32m     55\u001b[0m     epoch \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m offset\n\u001b[1;32m---> 56\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_ckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# data\u001b[39;00m\n","Cell \u001b[1;32mIn[7], line 24\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(epoch, model, optimizer, train_loader, save_step, save_ckpt_path, train_step)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03minputs = {'input_ids': batch[0],\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m          'attention_mask': batch[1],\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m  inputs['token_type_ids'] = batch[2]\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     20\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m: data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     21\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     22\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m: data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     23\u001b[0m           }\n\u001b[1;32m---> 24\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     28\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n","File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[1;32mC:\\sqlite\\mysql\\code\\AI\\FINAL_project\\dialogLM\\dialogLM\\model\\koelectra.py:73\u001b[0m, in \u001b[0;36mkoElectraForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m     55\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     56\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m         output_hidden_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     65\u001b[0m ):\n\u001b[0;32m     66\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m  labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`, defaults to :obj:`None`):\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03m      Labels for computing the sequence classification/regression loss.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m      If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m   discriminator_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43melectra\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m   sequence_output \u001b[38;5;241m=\u001b[39m discriminator_hidden_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     85\u001b[0m   logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output)\n","File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages\\transformers\\models\\electra\\modeling_electra.py:911\u001b[0m, in \u001b[0;36mElectraModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings_project\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    909\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings_project(hidden_states)\n\u001b[1;32m--> 911\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    912\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    913\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    914\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n","File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages\\transformers\\models\\electra\\modeling_electra.py:585\u001b[0m, in \u001b[0;36mElectraEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    574\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    575\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    576\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    582\u001b[0m         output_attentions,\n\u001b[0;32m    583\u001b[0m     )\n\u001b[0;32m    584\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 585\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    595\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n","File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages\\transformers\\models\\electra\\modeling_electra.py:474\u001b[0m, in \u001b[0;36mElectraLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    463\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    464\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    471\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    472\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    473\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 474\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    481\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages\\transformers\\models\\electra\\modeling_electra.py:401\u001b[0m, in \u001b[0;36mElectraAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    393\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    399\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    400\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 401\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    410\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    411\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages\\transformers\\models\\electra\\modeling_electra.py:331\u001b[0m, in \u001b[0;36mElectraSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    327\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(attention_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# This is actually dropping out entire tokens to attend to, which might\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;66;03m# seem a bit unusual, but is taken from the original Transformer paper.\u001b[39;00m\n\u001b[1;32m--> 331\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_probs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;66;03m# Mask heads if we want to\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages\\torch\\nn\\modules\\dropout.py:58\u001b[0m, in \u001b[0;36mDropout.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\myvenv\\lib\\site-packages\\torch\\nn\\functional.py:1266\u001b[0m, in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   1264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[0;32m   1265\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# root_path='drive/My Drive/Colab Notebooks/dialogLM'\n","root_path='C:\\\\sqlite\\\\mysql\\\\code\\\\AI\\\\FINAL_project\\\\dialogLM\\\\dialogLM'\n","data_path = f\"{root_path}\\\\data\\\\wellness_dialog_for_text_classification.txt\"\n","checkpoint_path =f\"{root_path}\\\\checkpoint\"\n","save_ckpt_path = f\"{checkpoint_path}\\\\koelectra-wellnesee-text-classification.pth\"\n","model_name_or_path = \"monologg/koelectra-base-discriminator\"\n","\n","\n","n_epoch = 50          # Num of Epoch\n","batch_size = 16      # 배치 사이즈\n","ctx = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","device = torch.device(ctx)\n","save_step = 100 # 학습 저장 주기\n","learning_rate = 5e-6  # Learning Rate\n","\n","# Electra Tokenizer\n","tokenizer = ElectraTokenizer.from_pretrained(model_name_or_path)\n","\n","# WellnessTextClassificationDataset 데이터 로더\n","dataset = WellnessTextClassificationDataset(file_path=data_path, tokenizer=tokenizer, device=device)\n","train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","electra_config = ElectraConfig.from_pretrained(model_name_or_path)\n","model = koElectraForSequenceClassification.from_pretrained(pretrained_model_name_or_path=model_name_or_path,\n","                                                            config=electra_config,\n","                                                            num_labels=359)\n","model.to(device)\n","\n","# Prepare optimizer and schedule (linear warmup and decay)\n","no_decay = ['bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","      'weight_decay': 0.01},\n","    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","]\n","optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n","\n","pre_epoch, pre_loss, train_step = 0, 0, 0\n","if os.path.isfile(save_ckpt_path):\n","    checkpoint = torch.load(save_ckpt_path, map_location=device)\n","    pre_epoch = checkpoint['epoch']\n","    pre_loss = checkpoint['loss']\n","    train_step =  checkpoint['train_step']\n","    total_train_step =  checkpoint['total_train_step']\n","\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","\n","    print(f\"load pretrain from: {save_ckpt_path}, epoch={pre_epoch}, loss={pre_loss}\")\n","    # best_epoch += 1\n","\n","losses = []\n","offset = pre_epoch\n","for step in range(n_epoch):\n","    epoch = step + offset\n","    loss = train( epoch, model, optimizer, train_loader, save_step, save_ckpt_path, train_step)\n","    losses.append(loss)\n","\n","# data\n","data = {\n","    \"loss\": losses\n","}\n","df = pd.DataFrame(data)\n","display(df)\n","\n","# graph\n","plt.figure(figsize=[12, 4])\n","plt.plot(losses, label=\"loss\")\n","plt.legend()\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNYyiHekDlXODvlaGIdYIOM","collapsed_sections":[],"machine_shape":"hm","name":"train-koelectra-for-wellness.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.18"}},"nbformat":4,"nbformat_minor":0}
